{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec06b2bf-e107-4879-9078-2c99677237a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80fd3143-b0cc-4067-abff-a759e5349610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load the dataset\n",
    "# Specify dtype for relevant columns to avoid warnings\n",
    "dtype = {'abstract': str}  # You can specify more columns if needed\n",
    "df = pd.read_csv('nyt-metadata.csv', dtype=dtype, usecols=['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2b746f9-c318-42aa-b230-9d722930d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows\n",
      "                                            abstract\n",
      "0  Article on upcoming New York Giants-Dallas Cow...\n",
      "1  Jeanne C Pond letter expresses hope that spiri...\n",
      "2  Many experts on Y2K computer problem report th...\n",
      "3  WILL the forces of globalism continue to push ...\n",
      "4   SPECIAL TODAY  The Millennium  Envisioning th...\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Inspect the dataset\n",
    "print(\"Sample rows\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1da0e40-a1cb-41af-90ed-f91aea4c351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names\n",
      "Index(['abstract'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column Names\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cc31d26-7314-42a3-819d-791a4d5298f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31917\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the 'abstract' column\n",
    "print(df['abstract'].isna().sum())\n",
    "\n",
    "# Drop rows with missing values in the text column\n",
    "df = df.dropna(subset=['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3b4e2b5-6bb8-4eee-8700-46d47364e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current no. of rows in df is:  2155709\n"
     ]
    }
   ],
   "source": [
    "print(\"current no. of rows in df is: \",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcb1e7ae-9b15-4810-a7bc-003e4b5e30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Download stopwords and lemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad4faf73-71ac-4a01-908e-2737c8917f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a38989f-fdad-4c78-ac02-9716680b1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Preprocess the text\n",
    "def preprocess(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)  # Tokenization and normalization\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Stop word removal\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "processed_docs = [preprocess(doc) for doc in df['abstract']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae02ca95-f179-4cd6-a40a-ea54c8592b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Documents:\n",
      "Document 1: ['article', 'upcoming', 'new', 'york', 'giant', 'dallas', 'cowboy', 'game', 'photo']\n",
      "Document 2: ['jeanne', 'pond', 'letter', 'express', 'hope', 'spiritual', 'development', 'artistic', 'knowledge', 'skill', 'self', 'esteem', 'flourish', 'new', 'century', 'drawing']\n",
      "Document 3: ['many', 'expert', 'computer', 'problem', 'report', 'internet', 'performed', 'impressively', 'rollover', 'even', 'sag', 'time', 'isolated', 'site', 'user', 'turned', 'mail', 'message', 'web', 'site', 'newsgroups', 'electronic', 'chat', 'room', 'track', 'arrival', 'year', 'time', 'zone']\n",
      "Document 4: ['force', 'globalism', 'continue', 'push', 'world', 'toward', 'american', 'style', 'capitalism', 'st', 'century', 'begin', 'advocate', 'free', 'market', 'doubt', 'economic', 'argument', 'socialism', 'dead', 'moreover', 'mean', 'creating', 'wealth', 'material', 'progress', 'american', 'capitalism', 'seems', 'clearly', 'superior', 'asian', 'variety', 'greater', 'level', 'government', 'planning', 'european', 'version', 'emphasis', 'social', 'welfare', 'protection', 'worker', 'losing', 'job']\n",
      "Document 5: ['special', 'today', 'millennium', 'envisioning', 'future', 'politics', 'people', 'city', 'machine', 'even', 'front', 'page', 'time', 'reflecting', 'past', 'section']\n"
     ]
    }
   ],
   "source": [
    "# Print processed documents (first 5)\n",
    "print(\"\\nProcessed Documents:\")\n",
    "for i, doc in enumerate(processed_docs[:5]):\n",
    "    print(f\"Document {i + 1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7fd17ec-7d33-44a8-91e0-bd9161d9fd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create a dictionary and corpus\n",
    "dictionary = Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0161432-5056-4f7a-963e-6513f35efc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens in the dictionary: 370663\n"
     ]
    }
   ],
   "source": [
    "# Display dictionary\n",
    "#print(\"\\nDictionary:\")\n",
    "#print(dictionary.token2id)\n",
    "# Print the number of unique tokens in the dictionary\n",
    "print(f\"Number of unique tokens in the dictionary: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c55d54db-fc96-4aee-97fb-42089ac7f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus:\n",
      "Document 1: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]\n",
      "Document 2: [(5, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)]\n",
      "Document 3: [(24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 2), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1)]\n",
      "Document 4: [(10, 1), (51, 1), (52, 2), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1)]\n",
      "Document 5: [(28, 1), (44, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Display corpus\n",
    "print(\"\\nCorpus:\")\n",
    "for doc_id, doc in enumerate(corpus[:5]):\n",
    "    print(f\"Document {doc_id + 1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00486323-d81a-42c0-955c-68b5164a8268",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 7: Build the LDA model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m  \u001b[38;5;66;03m# You can adjust the number of topics\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mLdaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_topics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_topics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\gensim\\models\\ldamodel.py:520\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    518\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    519\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_as_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_numpy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    523\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\gensim\\models\\ldamodel.py:1005\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1001\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1003\u001b[0m         pass_, chunk_no \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[0;32m   1004\u001b[0m     )\n\u001b[1;32m-> 1005\u001b[0m     gammat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_estep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_alpha:\n\u001b[0;32m   1008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alpha(gammat, rho())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\gensim\\models\\ldamodel.py:767\u001b[0m, in \u001b[0;36mLdaModel.do_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m--> 767\u001b[0m gamma, sstats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollect_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    768\u001b[0m state\u001b[38;5;241m.\u001b[39msstats \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sstats\n\u001b[0;32m    769\u001b[0m state\u001b[38;5;241m.\u001b[39mnumdocs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# avoids calling len(chunk) on a generator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\nlpenv\\lib\\site-packages\\gensim\\models\\ldamodel.py:721\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    719\u001b[0m Elogthetad \u001b[38;5;241m=\u001b[39m dirichlet_expectation(gammad)\n\u001b[0;32m    720\u001b[0m expElogthetad \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(Elogthetad)\n\u001b[1;32m--> 721\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpElogthetad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpElogbetad\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m epsilon\n\u001b[0;32m    722\u001b[0m \u001b[38;5;66;03m# If gamma hasn't changed much, we're done.\u001b[39;00m\n\u001b[0;32m    723\u001b[0m meanchange \u001b[38;5;241m=\u001b[39m mean_absolute_difference(gammad, lastgamma)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 7: Build the LDA model\n",
    "num_topics = 15  # You can adjust the number of topics\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09205a9e-9d4b-4f0e-aa3f-9f36bf1190b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Topic-word distribution\n",
    "print(\"\\nTopic-Word Distribution:\")\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "topic_word_dist = {}\n",
    "for topic_num, words in topics:\n",
    "    topic_word_dist[topic_num] = {word: prob for word, prob in words}\n",
    "    print(f\"Topic {topic_num}: {topic_word_dist[topic_num]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600515bc-3cbb-496a-b844-eb37a165645a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Document-topic distribution\n",
    "print(\"\\nDocument-Topic Distribution:\")\n",
    "doc_topic_dist = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topics = lda_model.get_document_topics(doc)\n",
    "    doc_topic_dist.append(doc_topics)\n",
    "    print(f\"Document {i}: {doc_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97782921-fc78-42bd-b031-0130840cbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Conditional probabilities for each word in each document\n",
    "print(\"\\nConditional Probabilities for each word in each document:\")\n",
    "conditional_probs = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_conditional_probs = []\n",
    "    for word_id, freq in doc:\n",
    "        word = dictionary[word_id]\n",
    "        word_conditional_probs = []\n",
    "        for topic_num in range(num_topics):\n",
    "            prob = lda_model.get_term_topics(word_id, minimum_probability=0)[topic_num][1]\n",
    "            word_conditional_probs.append((topic_num, prob))\n",
    "        doc_conditional_probs.append((word, word_conditional_probs))\n",
    "    conditional_probs.append(doc_conditional_probs)\n",
    "    print(f\"Document {i}: {doc_conditional_probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befd38b-f9b8-4174-9099-f917beffaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11: Topic distribution for each document\n",
    "print(\"\\nTopic Distribution for each document:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topic_dist = lda_model.get_document_topics(doc)\n",
    "    print(f\"Document {i}: {doc_topic_dist}\")\n",
    "\n",
    "# Step 12: Display the results in a tabular format using pandas\n",
    "# Topic-word distribution\n",
    "topic_word_df = pd.DataFrame(topic_word_dist).fillna(0)\n",
    "print(\"\\nTopic-Word Distribution (Tabular):\")\n",
    "print(topic_word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e904d-4b70-4f71-bbb6-af77caa17c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document-topic distribution\n",
    "doc_topic_df = pd.DataFrame([[prob for _, prob in lda_model.get_document_topics(doc, minimum_probability=0)] for doc in corpus])\n",
    "doc_topic_df.columns = [f'Topic {i}' for i in range(num_topics)]\n",
    "print(\"\\nDocument-Topic Distribution (Tabular):\")\n",
    "print(doc_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ff533-50ce-4060-8322-d88764e4ab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional probabilities for each word in each document\n",
    "conditional_probs_df = []\n",
    "for i, doc in enumerate(conditional_probs):\n",
    "    for word, probs in doc:\n",
    "        for topic_num, prob in probs:\n",
    "            conditional_probs_df.append((f'Doc {i}', word, f'Topic {topic_num}', prob))\n",
    "conditional_probs_df = pd.DataFrame(conditional_probs_df, columns=['Document', 'Word', 'Topic', 'Probability'])\n",
    "print(\"\\nConditional Probabilities (Tabular):\")\n",
    "print(conditional_probs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286faee9-ad92-4c0f-8641-d2804c0d4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13: Visualize the topics using pyLDAvis\n",
    "# Prepare the pyLDAvis data\n",
    "vis_data = gensimvis.prepare(lda_model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bb818-49f8-457b-bcb4-3617e274d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the visualization to an HTML file\n",
    "pyLDAvis.save_html(vis_data, 'nyt_lda_visualization.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f6042-cfdd-4e92-936f-94d566ec5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the visualization in the notebook\n",
    "pyLDAvis.display(vis_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89be1fd1-f191-4a51-bf51-d48b183bc430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70b2c010-2f19-40a7-8c37-179bdd5e2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Sample data\n",
    "documents = [\n",
    "    \"Natural language processing is a fascinating field of study.\",\n",
    "    \"Machine learning and deep learning are subsets of artificial intelligence.\",\n",
    "    \"Text mining involves the process of extracting meaningful information from text.\",\n",
    "    \"Topic modeling is a technique for discovering abstract topics within a collection of documents.\",\n",
    "    \"LDA is a popular topic modeling algorithm in the field of NLP.\",\n",
    "    \"Deep learning has revolutionized the field of artificial intelligence.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09f72ff-69e7-43f9-8145-3cc49601296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7169203b-346f-4928-bc65-7ff6c7912cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f440c9f-1067-476b-b027-4ac86438ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Preprocess the Text\n",
    "def preprocess(text):\n",
    "    tokens = simple_preprocess(text, deacc=True)  # Tokenization and normalization\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Stop word removal\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "processed_docs = [preprocess(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b98b26-d38b-4a72-a55d-d8571e325cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed Documents:\n",
      "Document 1: ['natural', 'language', 'processing', 'fascinating', 'field', 'study']\n",
      "Document 2: ['machine', 'learning', 'deep', 'learning', 'subsets', 'artificial', 'intelligence']\n",
      "Document 3: ['text', 'mining', 'involves', 'process', 'extracting', 'meaningful', 'information', 'text']\n",
      "Document 4: ['topic', 'modeling', 'technique', 'discovering', 'abstract', 'topics', 'within', 'collection', 'documents']\n",
      "Document 5: ['lda', 'popular', 'topic', 'modeling', 'algorithm', 'field', 'nlp']\n",
      "Document 6: ['deep', 'learning', 'revolutionized', 'field', 'artificial', 'intelligence']\n"
     ]
    }
   ],
   "source": [
    "# Print processed documents\n",
    "print(\"\\nProcessed Documents:\")\n",
    "for i, doc in enumerate(processed_docs):\n",
    "    print(f\"Document {i + 1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4488eec3-f76c-44d8-abb0-85e3d3bfe1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create a dictionary and corpus\n",
    "dictionary = Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7dddaf-7e56-4323-a6f2-c3e31c630449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dictionary:\n",
      "{'fascinating': 0, 'field': 1, 'language': 2, 'natural': 3, 'processing': 4, 'study': 5, 'artificial': 6, 'deep': 7, 'intelligence': 8, 'learning': 9, 'machine': 10, 'subsets': 11, 'extracting': 12, 'information': 13, 'involves': 14, 'meaningful': 15, 'mining': 16, 'process': 17, 'text': 18, 'abstract': 19, 'collection': 20, 'discovering': 21, 'documents': 22, 'modeling': 23, 'technique': 24, 'topic': 25, 'topics': 26, 'within': 27, 'algorithm': 28, 'lda': 29, 'nlp': 30, 'popular': 31, 'revolutionized': 32}\n"
     ]
    }
   ],
   "source": [
    "# Display dictionary\n",
    "print(\"\\nDictionary:\")\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf3a1e6-161a-4e08-b486-04b757df83cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus:\n",
      "Document 1: [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]\n",
      "Document 2: [(6, 1), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1)]\n",
      "Document 3: [(12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 2)]\n",
      "Document 4: [(19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)]\n",
      "Document 5: [(1, 1), (23, 1), (25, 1), (28, 1), (29, 1), (30, 1), (31, 1)]\n",
      "Document 6: [(1, 1), (6, 1), (7, 1), (8, 1), (9, 1), (32, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Display corpus\n",
    "print(\"\\nCorpus:\")\n",
    "for doc_id, doc in enumerate(corpus):\n",
    "    print(f\"Document {doc_id + 1}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ff8dfc-be17-410f-bca8-8698762e356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Build the LDA model\n",
    "num_topics = 3\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f46d7fe-e4d7-45d5-85eb-ba65f3e59c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic-Word Distribution:\n",
      "Topic 0: {'field': 0.07617569, 'processing': 0.075079665, 'fascinating': 0.07493144, 'study': 0.07486473, 'language': 0.07442345, 'natural': 0.07428721, 'text': 0.022472566, 'extracting': 0.0217278, 'involves': 0.021021795, 'process': 0.020985322}\n",
      "Topic 1: {'learning': 0.09580206, 'intelligence': 0.0667359, 'deep': 0.06666222, 'artificial': 0.06646762, 'field': 0.039246384, 'topic': 0.038355812, 'machine': 0.03829403, 'subsets': 0.038262185, 'abstract': 0.038261633, 'revolutionized': 0.03822168}\n",
      "Topic 2: {'text': 0.07931718, 'modeling': 0.05485446, 'field': 0.054565594, 'topic': 0.054432344, 'nlp': 0.053706195, 'algorithm': 0.05359196, 'popular': 0.053562265, 'lda': 0.053146146, 'meaningful': 0.046936154, 'process': 0.0456014}\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Topic-word distribution\n",
    "print(\"\\nTopic-Word Distribution:\")\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "topic_word_dist = {}\n",
    "for topic_num, words in topics:\n",
    "    topic_word_dist[topic_num] = {word: prob for word, prob in words}\n",
    "    print(f\"Topic {topic_num}: {topic_word_dist[topic_num]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c93aa77b-4be4-4f7c-9021-76177c2d8896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document-Topic Distribution:\n",
      "Document 0: [(0, 0.90296334), (1, 0.04835667), (2, 0.048679963)]\n",
      "Document 1: [(0, 0.04207211), (1, 0.9159677), (2, 0.041960176)]\n",
      "Document 2: [(0, 0.037648376), (1, 0.038033746), (2, 0.9243179)]\n",
      "Document 3: [(0, 0.03388926), (1, 0.93064), (2, 0.03547071)]\n",
      "Document 4: [(0, 0.043428745), (1, 0.043789327), (2, 0.9127819)]\n",
      "Document 5: [(0, 0.050577495), (1, 0.89983165), (2, 0.04959086)]\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Document-topic distribution\n",
    "print(\"\\nDocument-Topic Distribution:\")\n",
    "doc_topic_dist = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topics = lda_model.get_document_topics(doc)\n",
    "    doc_topic_dist.append(doc_topics)\n",
    "    print(f\"Document {i}: {doc_topics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9713d674-d49a-485f-8731-2dac93a23154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conditional Probabilities for each word in each document:\n",
      "Document 0: [('fascinating', [(0, 0.050226413), (1, 0.0015088858), (2, 0.002024885)]), ('field', [(0, 0.05145469), (1, 0.02625679), (2, 0.036553912)]), ('language', [(0, 0.049725533), (1, 0.0015475627), (2, 0.0021446473)]), ('natural', [(0, 0.04959125), (1, 0.0014811677), (2, 0.0022877285)]), ('processing', [(0, 0.050372623), (1, 0.0014498696), (2, 0.0020572364)]), ('study', [(0, 0.050160613), (1, 0.0014648661), (2, 0.0021093497)])]\n",
      "Document 1: [('artificial', [(0, 0.0032517237), (1, 0.053283826), (2, 0.0025308228)]), ('deep', [(0, 0.0032583605), (1, 0.05347902), (2, 0.0023859714)]), ('intelligence', [(0, 0.003171573), (1, 0.05355291), (2, 0.0023964662)]), ('learning', [(0, 0.0032791658), (1, 0.08282172), (2, 0.002440297)]), ('machine', [(0, 0.0028751972), (1, 0.025329415), (2, 0.0020620103)]), ('subsets', [(0, 0.00291496), (1, 0.02529845), (2, 0.0020549772)])]\n",
      "Document 2: [('extracting', [(0, 0.003881533), (1, 0.0070027392), (2, 0.023279727)]), ('information', [(0, 0.0034226421), (1, 0.004956923), (2, 0.027385382)]), ('involves', [(0, 0.0035066293), (1, 0.005322434), (2, 0.026617391)]), ('meaningful', [(0, 0.0033233296), (1, 0.004092852), (2, 0.029137371)]), ('mining', [(0, 0.0034564873), (1, 0.0056572296), (2, 0.026094185)]), ('process', [(0, 0.0034876957), (1, 0.004654134), (2, 0.027854493)]), ('text', [(0, 0.0042938236), (1, 0.008177531), (2, 0.061158616)])]\n",
      "Document 3: [('abstract', [(0, 0.0027858794), (1, 0.02529791), (2, 0.0021486087)]), ('collection', [(0, 0.0028931666), (1, 0.025177125), (2, 0.0021545654)]), ('discovering', [(0, 0.0028066211), (1, 0.025066886), (2, 0.0022968652)]), ('documents', [(0, 0.0027730118), (1, 0.025180608), (2, 0.0022408764)]), ('modeling', [(0, 0.0034035973), (1, 0.025077773), (2, 0.036837023)]), ('technique', [(0, 0.0029218148), (1, 0.02486184), (2, 0.0023584159)]), ('topic', [(0, 0.003383456), (1, 0.02538952), (2, 0.036423355)]), ('topics', [(0, 0.0028501544), (1, 0.025128094), (2, 0.0022205866)]), ('within', [(0, 0.002928936), (1, 0.024737328), (2, 0.0024443557)])]\n",
      "Document 4: [('field', [(0, 0.05145469), (1, 0.02625679), (2, 0.036553912)]), ('modeling', [(0, 0.0034035973), (1, 0.025077773), (2, 0.036837023)]), ('topic', [(0, 0.003383456), (1, 0.02538952), (2, 0.036423355)]), ('algorithm', [(0, 0.003024159), (1, 0.0014496029), (2, 0.035600755)]), ('lda', [(0, 0.003259116), (1, 0.0014853596), (2, 0.035164893)]), ('nlp', [(0, 0.0029774215), (1, 0.0014344788), (2, 0.035712503)]), ('popular', [(0, 0.003016415), (1, 0.0014633965), (2, 0.0355717)])]\n",
      "Document 5: [('field', [(0, 0.05145469), (1, 0.02625679), (2, 0.036553912)]), ('artificial', [(0, 0.0032517237), (1, 0.053283826), (2, 0.0025308228)]), ('deep', [(0, 0.0032583605), (1, 0.05347902), (2, 0.0023859714)]), ('intelligence', [(0, 0.003171573), (1, 0.05355291), (2, 0.0023964662)]), ('learning', [(0, 0.0032791658), (1, 0.08282172), (2, 0.002440297)]), ('revolutionized', [(0, 0.0029124727), (1, 0.025259055), (2, 0.0020837951)])]\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Conditional probabilities for each word in each document\n",
    "print(\"\\nConditional Probabilities for each word in each document:\")\n",
    "conditional_probs = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_conditional_probs = []\n",
    "    for word_id, freq in doc:\n",
    "        word = dictionary[word_id]\n",
    "        word_conditional_probs = []\n",
    "        for topic_num in range(num_topics):\n",
    "            prob = lda_model.get_term_topics(word_id, minimum_probability=0)[topic_num][1]\n",
    "            word_conditional_probs.append((topic_num, prob))\n",
    "        doc_conditional_probs.append((word, word_conditional_probs))\n",
    "    conditional_probs.append(doc_conditional_probs)\n",
    "    print(f\"Document {i}: {doc_conditional_probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b1ea00-9d31-49d0-b5ae-6035e093929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic Distribution for each document:\n",
      "Document 0: [(0, 0.90296745), (1, 0.04835655), (2, 0.048675966)]\n",
      "Document 1: [(0, 0.042072743), (1, 0.91596746), (2, 0.04195976)]\n",
      "Document 2: [(0, 0.03764845), (1, 0.038036905), (2, 0.9243146)]\n",
      "Document 3: [(0, 0.03388926), (1, 0.93064), (2, 0.03547073)]\n",
      "Document 4: [(0, 0.043428488), (1, 0.043789923), (2, 0.9127816)]\n",
      "Document 5: [(0, 0.050557405), (1, 0.8998589), (2, 0.04958371)]\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Topic distribution for each document\n",
    "print(\"\\nTopic Distribution for each document:\")\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topic_dist = lda_model.get_document_topics(doc)\n",
    "    print(f\"Document {i}: {doc_topic_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78cd9676-e9cc-4c67-b8aa-94747edc62fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic-Word Distribution (Tabular):\n",
      "                       0         1         2\n",
      "field           0.076176  0.039246  0.054566\n",
      "processing      0.075080  0.000000  0.000000\n",
      "fascinating     0.074931  0.000000  0.000000\n",
      "study           0.074865  0.000000  0.000000\n",
      "language        0.074423  0.000000  0.000000\n",
      "natural         0.074287  0.000000  0.000000\n",
      "text            0.022473  0.000000  0.079317\n",
      "extracting      0.021728  0.000000  0.000000\n",
      "involves        0.021022  0.000000  0.000000\n",
      "process         0.020985  0.000000  0.045601\n",
      "learning        0.000000  0.095802  0.000000\n",
      "intelligence    0.000000  0.066736  0.000000\n",
      "deep            0.000000  0.066662  0.000000\n",
      "artificial      0.000000  0.066468  0.000000\n",
      "topic           0.000000  0.038356  0.054432\n",
      "machine         0.000000  0.038294  0.000000\n",
      "subsets         0.000000  0.038262  0.000000\n",
      "abstract        0.000000  0.038262  0.000000\n",
      "revolutionized  0.000000  0.038222  0.000000\n",
      "modeling        0.000000  0.000000  0.054854\n",
      "nlp             0.000000  0.000000  0.053706\n",
      "algorithm       0.000000  0.000000  0.053592\n",
      "popular         0.000000  0.000000  0.053562\n",
      "lda             0.000000  0.000000  0.053146\n",
      "meaningful      0.000000  0.000000  0.046936\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Display the results in a tabular format using pandas\n",
    "# Topic-word distribution\n",
    "topic_word_df = pd.DataFrame(topic_word_dist).fillna(0)\n",
    "print(\"\\nTopic-Word Distribution (Tabular):\")\n",
    "print(topic_word_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "171de530-adfe-460e-9686-55d856dbdace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document-Topic Distribution (Tabular):\n",
      "    Topic 0   Topic 1   Topic 2\n",
      "0  0.902967  0.048356  0.048676\n",
      "1  0.042072  0.915968  0.041960\n",
      "2  0.037648  0.038035  0.924317\n",
      "3  0.033889  0.930640  0.035470\n",
      "4  0.043428  0.043789  0.912783\n",
      "5  0.050554  0.899862  0.049584\n"
     ]
    }
   ],
   "source": [
    "# Document-topic distribution\n",
    "doc_topic_df = pd.DataFrame([[prob for _, prob in lda_model.get_document_topics(doc, minimum_probability=0)] for doc in corpus])\n",
    "doc_topic_df.columns = [f'Topic {i}' for i in range(num_topics)]\n",
    "print(\"\\nDocument-Topic Distribution (Tabular):\")\n",
    "print(doc_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b3524e6-4311-4caa-a252-b715c908a474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conditional Probabilities (Tabular):\n",
      "    Document            Word    Topic  Probability\n",
      "0      Doc 0     fascinating  Topic 0     0.050226\n",
      "1      Doc 0     fascinating  Topic 1     0.001509\n",
      "2      Doc 0     fascinating  Topic 2     0.002025\n",
      "3      Doc 0           field  Topic 0     0.051455\n",
      "4      Doc 0           field  Topic 1     0.026257\n",
      "..       ...             ...      ...          ...\n",
      "118    Doc 5        learning  Topic 1     0.082822\n",
      "119    Doc 5        learning  Topic 2     0.002440\n",
      "120    Doc 5  revolutionized  Topic 0     0.002912\n",
      "121    Doc 5  revolutionized  Topic 1     0.025259\n",
      "122    Doc 5  revolutionized  Topic 2     0.002084\n",
      "\n",
      "[123 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Conditional probabilities for each word in each document\n",
    "conditional_probs_df = []\n",
    "for i, doc in enumerate(conditional_probs):\n",
    "    for word, probs in doc:\n",
    "        for topic_num, prob in probs:\n",
    "            conditional_probs_df.append((f'Doc {i}', word, f'Topic {topic_num}', prob))\n",
    "conditional_probs_df = pd.DataFrame(conditional_probs_df, columns=['Document', 'Word', 'Topic', 'Probability'])\n",
    "print(\"\\nConditional Probabilities (Tabular):\")\n",
    "print(conditional_probs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea391a9-08df-4fbb-88f5-2403d24626d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
